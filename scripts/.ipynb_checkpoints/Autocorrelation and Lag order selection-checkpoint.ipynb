{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import acf, ccf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from Swing import Swing\n",
    "from Swing.util.Evaluator import Evaluator\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "from nxpd import draw\n",
    "from nxpd import nxpdParams\n",
    "nxpdParams['show'] = 'ipynb'\n",
    "\n",
    "sys.path.append(\"../pipelines\")\n",
    "import Pipelines as tdw\n",
    "\n",
    "def get_experiment_list(filename):\n",
    "    # load files\n",
    "    timecourse = pd.read_csv(filename, sep=\"\\t\")\n",
    "    # divide into list of dataframes\n",
    "    experiments = []\n",
    "    for i in range(0,85,21):\n",
    "        experiments.append(timecourse.ix[i:i+20])\n",
    "    #reformat\n",
    "    for idx,exp in enumerate(experiments):\n",
    "        exp = exp.set_index('Time')\n",
    "        experiments[idx]=exp\n",
    "    return(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_folder = \"/projects/p20519/roller_output/optimizing_window_size/RandomForest/insilico_size10_1/\"\n",
    "\n",
    "output_path = \"/home/jjw036/Roller/insilico_size10_1\"\n",
    "\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "save_path = ('./window_size_selection_swing_results.pickle')\n",
    "\n",
    "data_folder = \"../output/insilico_size10_1\"\n",
    "file_path = \"../data/dream4/insilico_size10_1_timeseries.tsv\"\n",
    "run_params = {'data_folder': data_folder,\n",
    "              'file_path':file_path,\n",
    "              'td_window':10,\n",
    "              'min_lag':1,\n",
    "              'max_lag':3,\n",
    "              'n_trees':10,\n",
    "              'permutation_n':10,\n",
    "              'lag_method':'mean_mean',\n",
    "              'calc_mse':False,\n",
    "              'bootstrap_n':100,\n",
    "              'n_trials':1,\n",
    "              'run_time':current_time,\n",
    "              'sort_by':'adj',\n",
    "              'iterating_param':'td_window',\n",
    "              }\n",
    "\n",
    "try:\n",
    "    tdr = pd.read_pickle(save_path)\n",
    "except:\n",
    "    roc,pr, tdr = tdw.get_td_stats(**run_params)\n",
    "    pd.to_pickle(tdr, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list of nodes = G1..G10\n",
    "nodes = ['G'+str(x) for x in range(1,11)]\n",
    "#convert edge list to list of tuples\n",
    "edges = pd.read_csv(\"../data/dream4/insilico_size10_1_goldstandard.tsv\",sep=\"\\t\",header=None)\n",
    "edges = edges[edges[2] > 0]\n",
    "edges=edges[edges.columns[0:2]]\n",
    "edges = [tuple(x) for x in edges.values]\n",
    "G = nx.DiGraph()\n",
    "G.graph['rankdir'] = 'LR'\n",
    "G.graph['dpi'] = 50\n",
    "\n",
    "\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "#examples of other kinds of drawing\n",
    "#G.add_node(0, color='red', style='filled', fillcolor='pink')\n",
    "#G.add_node(1, shape='square')\n",
    "#G.add_node(3, style='filled', fillcolor='#00ffff')\n",
    "#G.add_edge(0, 1, color='red', style='dashed')\n",
    "#G.add_edge(3, 3, label='a')\n",
    "draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiments=get_experiment_list(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ce = 3\n",
    "for gene in tdr.gene_list:\n",
    "    auto = acf(experiments[ce][gene])\n",
    "    x = stats.linregress(tdr.time_vec[:10], auto[:10])\n",
    "    p_val = x[3]*len(tdr.gene_list)\n",
    "    if p_val < 0.05:\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ln1= ax1.plot(tdr.time_vec, auto, 'o-', label='acf')\n",
    "        ax2 = ax1.twinx()\n",
    "        ln2 = ax2.plot(tdr.time_vec, (experiments[ce][gene]), 'o-', c='r', label='data')\n",
    "        lns = ln1+ln2\n",
    "        labs = [l.get_label() for l in lns]\n",
    "        ax2.legend(lns, labs, loc='best')\n",
    "        plt.title(gene)\n",
    "\n",
    "# This may represent the genes that should be used as child nodes in the regression for THIS experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_gold_standard = file_path.replace(\"timeseries.tsv\",\"goldstandard.tsv\")\n",
    "evaluator = Evaluator(current_gold_standard, '\\t')\n",
    "true_edges = evaluator.gs_flat.tolist()\n",
    "print(true_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,20))\n",
    "ii = 1\n",
    "parent = []\n",
    "child = []\n",
    "slope = []\n",
    "p_value = []\n",
    "for gene1 in tdr.gene_list:\n",
    "    for gene2 in tdr.gene_list:\n",
    "        current_ax = f.add_subplot(len(tdr.gene_list),len(tdr.gene_list) , ii)\n",
    "        if ((gene1, gene2) in true_edges):\n",
    "            color = 'b'\n",
    "        elif (((gene2, gene1) in true_edges)):\n",
    "            color = 'r'\n",
    "        else:\n",
    "            color = 'k'\n",
    "        unbiased = True\n",
    "        if (gene1 == gene2):\n",
    "            # Not clear why this is the case by statsmodels uses unbiased=False for acf\n",
    "            # unbiased = False for ccf yields the same results as acf\n",
    "            unbiased = False\n",
    "        parent.append(gene1)\n",
    "        child.append(gene2)\n",
    "        ccf_results = ccf(experiments[ce][gene1], experiments[ce][gene2], unbiased=unbiased)\n",
    "        x = stats.linregress(tdr.time_vec[:10], ccf_results[:10])\n",
    "        slope.append(x.slope)\n",
    "        p_value.append(x.pvalue*(len(tdr.gene_list)**2))\n",
    "        current_ax.plot(ccf_results, 'o-', c=color)\n",
    "        current_ax.set_title(gene1+','+gene2)\n",
    "        current_ax.set_ylim([-1, 1])\n",
    "        print(ii)\n",
    "        ii+=1\n",
    "plt.tight_layout()\n",
    "df = pd.DataFrame([parent, child, slope, p_value], index=['Parent', 'Child', 'Slope', 'Pval']).T\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This might be the list of edges that should even be considered\n",
    "df[(df['Pval']<0.05) &(df['Parent']!=df['Child'])]\n",
    "df['Abs_slope'] = np.abs(df['Slope'])\n",
    "df.sort(columns='Abs_slope', inplace=True, ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are cross-correlation and moving pearson the same?\n",
    "If formulated properly, they appear to be the same. See test example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([1,3,2,6])\n",
    "ai = a-a.mean()\n",
    "(np.correlate(ai,ai, 'full')[len(a)-1:]/np.array([4,3,2,1]))/np.std(a)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ccf(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.pearsonr([1,3,2,6], [1,3,2,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.pearsonr([3,2,6], [1,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.pearsonr([2,6], [1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_trunc = np.array([3,2,6])\n",
    "a_shift = np.array([1,3,2])\n",
    "ccov = np.mean((a_trunc-a.mean())*(a_shift-a.mean()))/(np.std(a)**2)\n",
    "ccov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_trunc = np.array([2,6])\n",
    "a_shift = np.array([1,3])\n",
    "ccov = np.mean((a_trunc-a.mean())*(a_shift-a.mean()))/(np.std(a)**2)\n",
    "ccov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How does pearson correlation compare to cross correlation\n",
    "g1 = 'G1'\n",
    "g2 = 'G2'\n",
    "######################################################################################################################\n",
    "#NOTE: If you don't take the values from the pd dataframe it messes things up\n",
    "######################################################################################################################\n",
    "g1_data = experiments[1][g1].values\n",
    "g2_data = experiments[1][g2].values\n",
    "ccf_results = ccf(g1_data, g2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vals = []\n",
    "for ii in range(len(g1_data)-1):\n",
    "    trunc_g1 = g1_data[ii:,]\n",
    "    if ii == 0:\n",
    "        shift_g2 = g2_data[:]\n",
    "    else:\n",
    "        shift_g2 = g2_data[:-ii,]\n",
    "    ccov = np.mean((trunc_g1-g1_data.mean())*(shift_g2-g2_data.mean()))\n",
    "    vals.append(ccov/(np.std(g1_data)*np.std(g2_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ccf_results, 'o')\n",
    "plt.plot(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at some real lags and see what is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=500)\n",
    "def identify_lags(experiments, true_edges, swing_obj, perturb_idx=0, auto_len=10, p_thresh=0.05):\n",
    "    for ee, experiment in enumerate(experiments):\n",
    "        for parent, child in true_edges:\n",
    "            data_df = pd.DataFrame()\n",
    "            data_df['Parent'] = experiment[parent]\n",
    "            data_df['Child'] = experiment[child]\n",
    "            auto_parent = acf(data_df['Parent'])\n",
    "            auto_child = acf(data_df['Child'])\n",
    "            pval_parent = stats.linregress(data_df.index[:auto_len], auto_parent[:auto_len]).pvalue*len(swing_obj.gene_list)\n",
    "            pval_child = stats.linregress(data_df.index[:auto_len], auto_child[:auto_len]).pvalue*len(swing_obj.gene_list)\n",
    "            if pval_parent < p_thresh and pval_child < p_thresh:\n",
    "                perturb_df = data_df.iloc[perturb_idx:]\n",
    "                f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "                ax1.plot(perturb_df.index,  perturb_df['Parent'], 'o-', label=parent)\n",
    "                ax1.plot(perturb_df.index,  perturb_df['Child'], 'o-', label=child)\n",
    "                ax1.set_title(ee)\n",
    "                ax1.legend(loc='best')\n",
    "                ccf_forward = ccf(perturb_df['Parent'],  perturb_df['Child'])\n",
    "                ccf_reverse = ccf( perturb_df['Child'], perturb_df['Parent'])\n",
    "                diff = (ccf_forward-ccf_reverse)/range(1,len(perturb_df.index)+1)\n",
    "                diff = diff/np.max(np.abs(diff))\n",
    "                ax2.plot(perturb_df.index, ccf_forward, 'o-', c='c', label='forward')\n",
    "                ax2.plot(perturb_df.index, ccf_reverse, 'o-', c='m', label='reverse')\n",
    "                ax2.plot(perturb_df.index, diff, 'o-', c='k', label='diff')\n",
    "                ax2.set_ylim([-1, 1])\n",
    "                ax2.legend(loc='best')\n",
    "\n",
    "identify_lags(experiments, true_edges, tdr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
